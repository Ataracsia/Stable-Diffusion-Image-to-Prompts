{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights & Biasesを使って、CLIP-Vit-Largeモデルのハイパーパラメータチューニングを行うためのNotebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy import spatial\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import sys\n",
    "# pathの追加\n",
    "# sys.path.append('./sentence-transformers-2-2-2/sentence-transformers')\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "# sentence transformerの構築\n",
    "st_model = SentenceTransformer('./sentence-transformers-2-2-2/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "# clip_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('using', device)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_loss(pred, target):\n",
    "    # nn.CosineSimilarity()は二つのtensorが同じ向きであれば1を返す\n",
    "    cos = nn.CosineSimilarity(dim=1)\n",
    "    output = -cos(pred, target).mean()\n",
    "    return output\n",
    "\n",
    "def cosine_similarity(y_trues, y_preds):\n",
    "    # 1 - spa...は見た目的にはlossを求めているように見えるが，\n",
    "    # 実際には1 - (1 - cos_sim) = cos_simである．\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
    "    # を参照のこと\n",
    "    return np.mean([\n",
    "        1 - spatial.distance.cosine(y_true, y_pred) \n",
    "        for y_true, y_pred in zip(y_trues, y_preds)\n",
    "    ])\n",
    "\n",
    "def denoising(image_path, DENOISING=None, KERNEL=1):\n",
    "    if DENOISING == 'smooth':\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.blur(image, (KERNEL, KERNEL))\n",
    "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    elif DENOISING == 'median':\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.medianBlur(image, KERNEL)\n",
    "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    elif DENOISING == 'gaussian':\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.GaussianBlur(image, (KERNEL, KERNEL), 0)\n",
    "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        image = Image.open(image_path)\n",
    "    \n",
    "    return image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "train_df = pd.read_csv('./Datasets/2M/diffusiondb_154318.csv')\n",
    "train_df\n",
    "\n",
    "# validation data\n",
    "# https://www.kaggle.com/code/shoheiazuma/diffusiondb-data-cleansing/comments\n",
    "val_df1 = pd.read_csv('./Datasets/30K/modified_all_30K.csv')\n",
    "val_df2 = pd.read_csv('./Datasets/gustavosta-stable-diffusion-prompts-sd2-v2/cleansed_80K.csv')\n",
    "val_df3 = pd.read_csv('./Datasets/Generated/000000-006532/000000-006532_pairs.csv')\n",
    "val_df4 = pd.read_csv('./Datasets/Generated/006533-011027/006533-011027.csv')\n",
    "\n",
    "val_df1 = val_df1[['prompt', 'image_path']]\n",
    "val_df2 = val_df2[['prompt', 'image_path']]\n",
    "val_df3 = val_df3[['prompt', 'image_path']]\n",
    "val_df4 = val_df4[['prompt', 'image_path']]\n",
    "\n",
    "val_df = pd.concat([val_df1, val_df2, val_df3, val_df4],\n",
    "                   axis=0)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "val_df = val_df.sample(n=15000,\n",
    "                        random_state=42,\n",
    "                        axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a portrait of a female robot made from code, very intricate details, octane render, 8 k, trending on artstation\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "train_images = train_df['image_path'].tolist()\n",
    "train_prompts = train_df['prompt'].tolist()\n",
    "val_images = val_df['image_path'].tolist()\n",
    "val_prompts = val_df['prompt'].tolist()\n",
    "\n",
    "print(train_prompts[0])\n",
    "print(type(train_prompts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "(512, 512)\n"
     ]
    }
   ],
   "source": [
    "image = denoising(train_images[0], 'None', 3)\n",
    "print(type(image))\n",
    "print(image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image in val_images:\n",
    "#     Image.open(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512 at 0x238948C6A40>\n",
      "tensor([[[-0.5082, -0.4911, -0.5082,  ..., -0.5424, -0.5424, -0.5596],\n",
      "         [-0.4911, -0.4739, -0.4739,  ..., -0.5596, -0.5082, -0.5253],\n",
      "         [-0.4911, -0.5082, -0.4911,  ..., -0.5082, -0.5082, -0.5082],\n",
      "         ...,\n",
      "         [-0.7137, -0.6965, -0.7308,  ..., -1.4672, -0.3541,  0.0569],\n",
      "         [-0.7137, -0.7137, -0.7137,  ..., -1.5699, -0.7650,  0.0741],\n",
      "         [-0.7137, -0.7308, -0.6965,  ..., -1.7240, -1.2788, -0.0287]],\n",
      "\n",
      "        [[-0.3200, -0.3025, -0.3200,  ..., -0.3200, -0.3375, -0.3550],\n",
      "         [-0.3200, -0.3200, -0.3025,  ..., -0.3550, -0.3025, -0.3025],\n",
      "         [-0.3200, -0.3550, -0.3200,  ..., -0.3025, -0.3025, -0.3025],\n",
      "         ...,\n",
      "         [-0.5476, -0.5126, -0.5476,  ..., -1.2654, -0.5301, -0.3901],\n",
      "         [-0.5301, -0.5301, -0.5476,  ..., -1.2829, -0.7927, -0.3725],\n",
      "         [-0.5301, -0.5476, -0.5126,  ..., -1.4405, -1.1779, -0.3200]],\n",
      "\n",
      "        [[-0.1661, -0.1487, -0.1487,  ..., -0.1312, -0.1661, -0.2010],\n",
      "         [-0.1487, -0.1312, -0.1312,  ..., -0.1661, -0.1312, -0.1487],\n",
      "         [-0.1487, -0.1661, -0.1487,  ..., -0.1138, -0.1312, -0.1312],\n",
      "         ...,\n",
      "         [-0.3578, -0.3404, -0.3753,  ..., -0.9853, -0.5844, -0.5321],\n",
      "         [-0.3230, -0.3404, -0.3753,  ..., -0.9330, -0.7238, -0.4798],\n",
      "         [-0.3055, -0.3578, -0.3404,  ..., -1.0724, -1.0201, -0.4101]]])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(train_images[0])\n",
    "print(image)\n",
    "# image = clip_processor(images=image)['pixel_values'][0]\n",
    "# print(image.shape)\n",
    "image = transform(image)\n",
    "print(image)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_train_test_split():\n",
    "#     \"\"\"add your image paths and embedding labels here\"\"\"\n",
    "#     train_images = train_images\n",
    "#     train_labels = train_prompts\n",
    "#     test_images = val_images\n",
    "#     test_labels = val_prompts\n",
    "#     return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "# class IMGDataset(Dataset):\n",
    "#     def __init__(self, image_paths, targets, clip_processor=clip_processor, transform=transform):\n",
    "#         self.images = image_paths\n",
    "#         self.labels = targets\n",
    "#         self.input_processor = clip_processor\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         # image\n",
    "#         image = Image.open(self.images[item])\n",
    "#         image = self.transform(image)\n",
    "        \n",
    "#         # text\n",
    "#         target = self.labels[item]\n",
    "        \n",
    "#         return image, target\n",
    "\n",
    "# class Collator:\n",
    "#     def __init__(self):\n",
    "#         self.st_model = st_model\n",
    "    \n",
    "#     def __call__(self, batch):\n",
    "#         images, prompts = zip(*batch)\n",
    "#         images = torch.stack(images)\n",
    "#         embeddings = self.st_model.encode(prompts,\n",
    "#                                           show_progress_bar=False,\n",
    "#                                           convert_to_tensor=True)\n",
    "        \n",
    "#         return images, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        clip = AutoModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        self.vision = clip.vision_model\n",
    "        self.fc = nn.Linear(1024, 384)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.vision(x)['pooler_output']\n",
    "        return self.fc(out)\n",
    "\n",
    "def load_pretrained_model(UNFREEZE_START=0):\n",
    "    model = Net()\n",
    "\n",
    "    trainable_model_weights = False\n",
    "    for name, child in model.named_children():\n",
    "        if name == 'vision':\n",
    "            for pn, p in child.named_parameters():\n",
    "                if str(UNFREEZE_START) in pn:\n",
    "                    \"\"\"start unfreezing layer , the weights are trainable\"\"\"\n",
    "                    trainable_model_weights = True\n",
    "                p.requires_grad = trainable_model_weights\n",
    "                # if p.requires_grad:\n",
    "                #     print(f\"{pn} is set to be trainable.\")\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "構築するモデルのアーキテクチャ：  \n",
    "\n",
    "Net(  \n",
    "  (vision): CLIPVisionTransformer(  \n",
    "    (embeddings): CLIPVisionEmbeddings(  \n",
    "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)  \n",
    "      (position_embedding): Embedding(257, 1024)  \n",
    "    )  \n",
    "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)  \n",
    "    (encoder): CLIPEncoder(  \n",
    "      (layers): ModuleList(  \n",
    "        (0-23): 24 x CLIPEncoderLayer(  \n",
    "          (self_attn): CLIPAttention(  \n",
    "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)  \n",
    "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)  \n",
    "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)  \n",
    "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)  \n",
    "          )  \n",
    "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)  \n",
    "          (mlp): CLIPMLP(  \n",
    "            (activation_fn): QuickGELUActivation()  \n",
    "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)  \n",
    "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)  \n",
    "          )  \n",
    "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)  \n",
    "        )  \n",
    "      )  \n",
    "    )  \n",
    "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)  \n",
    "  )  \n",
    "  (fc): Linear(in_features=1024, out_features=384, bias=True)  \n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコードは、\n",
    "\n",
    "- [WandBでハイパラチューニングもしちゃおう！](https://qiita.com/ryo3568/items/5ef6ec8b5bba163bcf54)\n",
    "- [Weights & Biasesを使用したデータサイエンス実験管理](https://wandb.ai/wandb_fc/japanese/reports/Weights-Biases---Vmlldzo4MDI5MTA) - これは要復習\n",
    "\n",
    "を参考にした．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用する乱数のseedを固定\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweep Configulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Base\n",
    "# sweep_config = dict()\n",
    "\n",
    "# # metric\n",
    "# metric = dict()\n",
    "# metric['name'] = 'val_similarity'\n",
    "# metric['goal'] = 'maximize'\n",
    "# metric['target'] = 0.65\n",
    "\n",
    "# # parameters\n",
    "# parameters = dict()\n",
    "# parameters['epochs'] = {'value': 10}\n",
    "# parameters['batch_size'] = {'value': 30}\n",
    "# ## model parameter\n",
    "# parameters['unfreeze_start'] = {\n",
    "#     'distribution': 'int_uniform', \n",
    "#     'min': 21,\n",
    "#     'max': 22,\n",
    "# }\n",
    "# ## optimizer parameters\n",
    "# parameters['learning_rate'] = {\n",
    "#     'distribution': 'uniform',\n",
    "#     'min': 0.0030,\n",
    "#     'max': 0.0067\n",
    "# }\n",
    "# parameters['weight_decay'] = {\n",
    "#     'distribution': 'uniform',\n",
    "#     'min': 0.04,\n",
    "#     'max': 0.07\n",
    "# }\n",
    "\n",
    "# # early stopping\n",
    "# early_stopping = dict()\n",
    "# early_stopping['type'] = 'hyperband'\n",
    "# early_stopping['max_iter'] = 5\n",
    "# early_stopping['s'] = 3\n",
    "\n",
    "# sweep_config['method'] = 'bayes'\n",
    "# sweep_config['metric'] = metric\n",
    "# sweep_config['parameters'] = parameters\n",
    "# sweep_config['early_terminate'] = early_stopping\n",
    "# sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Base + Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Base + Denoising\n",
    "\n",
    "# sweep_config = dict()\n",
    "\n",
    "# # metric\n",
    "# metric = dict()\n",
    "# metric['name'] = 'val_similarity'\n",
    "# metric['goal'] = 'maximize'\n",
    "# metric['target'] = 0.99\n",
    "\n",
    "# # parameters\n",
    "# parameters = dict()\n",
    "# parameters['epochs'] = {'value': 1000}\n",
    "# parameters['batch_size'] = {'value': 30}\n",
    "\n",
    "# ## denoising parameters\n",
    "# # parameters['denoising'] = {\n",
    "# #     'distribution': 'categorical',\n",
    "# #     'values': [\n",
    "# #         # 'smooth',\n",
    "# #         # 'median',\n",
    "# #         'gaussian',\n",
    "# #         # 'None'\n",
    "# #     ]\n",
    "# # }\n",
    "# parameters['denoising'] = {'value': 'gaussian'}\n",
    "\n",
    "# # parameters['kernel'] = {\n",
    "# #     'distribution': 'categorical',\n",
    "# #     'values': [3, 5, 7, 9]\n",
    "# # }\n",
    "# parameters['kernel'] = {'value': 3}\n",
    "\n",
    "# ## model parameter\n",
    "# # parameters['unfreeze_start'] = {\n",
    "# #     'distribution': 'int_uniform', \n",
    "# #     'min': 21,\n",
    "# #     'max': 22,\n",
    "# # }\n",
    "# parameters['unfreeze_start'] = {'value': 21}\n",
    "\n",
    "# ## optimizer parameters\n",
    "# # parameters['learning_rate'] = {\n",
    "# #     'distribution': 'uniform',\n",
    "# #     'min': 0.003,\n",
    "# #     'max': 0.008\n",
    "# # }\n",
    "# parameters['learning_rate'] = {'value': 0.004836150520205524}\n",
    "\n",
    "# # parameters['weight_decay'] = {\n",
    "# #     'distribution': 'uniform',\n",
    "# #     'min': 0.04,\n",
    "# #     'max': 0.07\n",
    "# # }\n",
    "# parameters['weight_decay'] = {'value': 0.05458727423867926}\n",
    "\n",
    "# # early stopping\n",
    "# # early_stopping = dict()\n",
    "# # early_stopping['type'] = 'hyperband'\n",
    "# # early_stopping['max_iter'] = 5\n",
    "# # early_stopping['s'] = 2\n",
    "\n",
    "# sweep_config['method'] = 'bayes'\n",
    "# sweep_config['metric'] = metric\n",
    "# sweep_config['parameters'] = parameters\n",
    "# # sweep_config['early_terminate'] = early_stopping\n",
    "# sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Base + Denoising + Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'bayes',\n",
       " 'metric': {'name': 'COUNTER', 'goal': 'maximize', 'target': 4},\n",
       " 'parameters': {'epochs': {'value': 1000},\n",
       "  'batch_size': {'values': [30, 30]},\n",
       "  'denoising': {'value': 'gaussian'},\n",
       "  'kernel': {'value': 3},\n",
       "  'unfreeze_start': {'value': 21},\n",
       "  'learning_rate': {'value': 0.004836150520205524},\n",
       "  'weight_decay': {'value': 0.05458727423867926}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base + Denoising + Early Stopping\n",
    "\n",
    "sweep_config = dict()\n",
    "\n",
    "# metric\n",
    "metric = dict()\n",
    "metric['name'] = 'COUNTER'\n",
    "metric['goal'] = 'maximize'\n",
    "metric['target'] = 4\n",
    "\n",
    "# parameters\n",
    "parameters = dict()\n",
    "parameters['epochs'] = {'value': 1000}\n",
    "parameters['batch_size'] = {'values': [30, 30]}\n",
    "\n",
    "## denoising parameters\n",
    "# parameters['denoising'] = {\n",
    "#     'distribution': 'categorical',\n",
    "#     'values': [\n",
    "#         # 'smooth',\n",
    "#         # 'median',\n",
    "#         'gaussian',\n",
    "#         # 'None'\n",
    "#     ]\n",
    "# }\n",
    "parameters['denoising'] = {'value': 'gaussian'}\n",
    "\n",
    "# parameters['kernel'] = {\n",
    "#     'distribution': 'categorical',\n",
    "#     'values': [3, 5, 7, 9]\n",
    "# }\n",
    "parameters['kernel'] = {'value': 3}\n",
    "\n",
    "## model parameter\n",
    "# parameters['unfreeze_start'] = {\n",
    "#     'distribution': 'int_uniform', \n",
    "#     'min': 21,\n",
    "#     'max': 22,\n",
    "# }\n",
    "parameters['unfreeze_start'] = {'value': 21}\n",
    "\n",
    "## optimizer parameters\n",
    "# parameters['learning_rate'] = {\n",
    "#     'distribution': 'uniform',\n",
    "#     'min': 0.003,\n",
    "#     'max': 0.008\n",
    "# }\n",
    "parameters['learning_rate'] = {'value': 0.004836150520205524}\n",
    "\n",
    "# parameters['weight_decay'] = {\n",
    "#     'distribution': 'uniform',\n",
    "#     'min': 0.04,\n",
    "#     'max': 0.07\n",
    "# }\n",
    "parameters['weight_decay'] = {'value': 0.05458727423867926}\n",
    "\n",
    "# early stopping\n",
    "# early_stopping = dict()\n",
    "# early_stopping['type'] = 'hyperband'\n",
    "# early_stopping['max_iter'] = 5\n",
    "# early_stopping['s'] = 2\n",
    "\n",
    "sweep_config['method'] = 'bayes'\n",
    "sweep_config['metric'] = metric\n",
    "sweep_config['parameters'] = parameters\n",
    "# sweep_config['early_terminate'] = early_stopping\n",
    "sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### val_similarity >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # >=0.5\n",
    "# sweep_config = dict()\n",
    "\n",
    "# # metric\n",
    "# metric = dict()\n",
    "# metric['name'] = 'val_similarity'\n",
    "# metric['goal'] = 'maximize'\n",
    "# metric['target'] = 0.65\n",
    "\n",
    "# # parameters\n",
    "# parameters = dict()\n",
    "# parameters['epochs'] = {'value': 10}\n",
    "# parameters['batch_size'] = {'value': 30}\n",
    "# ## model parameter\n",
    "# parameters['unfreeze_start'] = {'value': 22}\n",
    "# ## optimizer parameters\n",
    "# parameters['learning_rate'] = {\n",
    "#     'distribution': 'uniform',\n",
    "#     'min': 0.003,\n",
    "#     'max': 0.007\n",
    "# }\n",
    "# parameters['weight_decay'] = {\n",
    "#     'distribution': 'uniform',\n",
    "#     'min': 0.05,\n",
    "#     'max': 0.07\n",
    "# }\n",
    "\n",
    "# # early stopping\n",
    "# early_stopping = dict()\n",
    "# early_stopping['type'] = 'hyperband'\n",
    "# early_stopping['max_iter'] = 5\n",
    "# early_stopping['s'] = 3\n",
    "\n",
    "# sweep_config['method'] = 'bayes'\n",
    "# sweep_config['metric'] = metric\n",
    "# sweep_config['parameters'] = parameters\n",
    "# sweep_config['early_terminate'] = early_stopping\n",
    "# sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts/discussion/399549#2210844 より：  \n",
    "0.9のモメンタムをもつSGDオプティマイザーは大きな画像データセットで使えるらしい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "課題として、どのデータのsimilarityが低く出るか、もしくは高く出るかも記録させるとよい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Base （間違えて部分的に書き換えてしまっている）\n",
    "# def train(config=None):\n",
    "#     with wandb.init(config=config):\n",
    "#         #------------------------------------------------------------------------------\n",
    "#         config = wandb.config\n",
    "#         EPOCHS = config.epochs\n",
    "#         BATCH_SIZE = config.batch_size\n",
    "#         UNFREEZE_START = config.unfreeze_start\n",
    "#         LEARNING_RATE = config.learning_rate\n",
    "#         WEIGHT_DECAY = config.weight_decay\n",
    "#         #------------------------------------------------------------------------------\n",
    "        \n",
    "#         model = load_pretrained_model(UNFREEZE_START)    \n",
    "#         optimizer = optim.AdamW(filter(lambda p: p.requires_grad,\n",
    "#                                        model.parameters()),\n",
    "#                                 lr=LEARNING_RATE,\n",
    "#                                 weight_decay=WEIGHT_DECAY\n",
    "#                                 # fused=True\n",
    "#                                 )\n",
    "#         optimizer.zero_grad()\n",
    "#         # if config.optimizer == 'sgd':\n",
    "#         #     optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9)\n",
    "#         # elif config.optimizer == 'adam':\n",
    "#         #     optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "#         criterion = torch.nn.CosineEmbeddingLoss()\n",
    "        \n",
    "#         random.seed(42)\n",
    "#         shuffled_train_index = random.sample(range(len(train_images)),\n",
    "#                                              k=len(train_images))\n",
    "        \n",
    "#         num_batchs = math.ceil(len(train_images) / BATCH_SIZE)\n",
    "        \n",
    "#         # train\n",
    "#         for epoch in range(EPOCHS):\n",
    "#             model.train()\n",
    "#             train_loss = 0\n",
    "#             train_similarity = 0\n",
    "            \n",
    "#             print(f'epoch: {epoch}')\n",
    "#             for i in tqdm(range(0, len(train_images), BATCH_SIZE)):\n",
    "#                 indices = shuffled_train_index[i:i+BATCH_SIZE]\n",
    "\n",
    "#                 # input data -image-\n",
    "#                 batch_images = []\n",
    "#                 for index in indices:\n",
    "#                     image = Image.open(train_images[index])\n",
    "#                     image = transform(image)\n",
    "#                     batch_images.append(image)\n",
    "#                 batch_images = torch.stack(batch_images)\n",
    "#                 batch_images = batch_images.to(device)\n",
    "\n",
    "#                 # input data -prompt-\n",
    "#                 prompt_list = []\n",
    "#                 for index in indices:\n",
    "#                     prompt_list.append(train_prompts[index])\n",
    "\n",
    "#                 targets = st_model.encode(prompt_list, \n",
    "#                                                 show_progress_bar=False,\n",
    "#                                                 convert_to_tensor=True)\n",
    "#                 preds = model(batch_images)\n",
    "\n",
    "#                 # back probagation\n",
    "#                 labels = torch.ones(targets.size(0)).to(device)\n",
    "#                 loss = criterion(preds, targets, labels)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 optimizer.zero_grad()\n",
    "                \n",
    "#                 similarity = cosine_similarity(\n",
    "#                     preds.detach().cpu().numpy(), \n",
    "#                     targets.detach().cpu().numpy()\n",
    "#                     )\n",
    "#                 train_loss += loss.item()\n",
    "#                 train_similarity += similarity\n",
    "#                 similarity = 0\n",
    "#             train_loss /= num_batchs\n",
    "#             train_similarity /= num_batchs\n",
    "#             print(f'train loss: {train_loss} ', end='')\n",
    "#             print(f'train similarity: {train_similarity}')\n",
    "            \n",
    "#             # val\n",
    "#             model.eval()\n",
    "#             val_loss = 0\n",
    "#             val_similarity = 0\n",
    "#             num_batchs = math.ceil(len(val_images) / BATCH_SIZE)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 for i in tqdm(range(0, len(val_images), BATCH_SIZE)):\n",
    "\n",
    "#                     # input data -image-\n",
    "#                     images = val_images[i:i+BATCH_SIZE]\n",
    "#                     batch_images = []\n",
    "#                     for image in images:\n",
    "#                         image = Image.open(image)\n",
    "#                         image = transform(image)\n",
    "#                         batch_images.append(image)\n",
    "#                     batch_images = torch.stack(batch_images)\n",
    "#                     batch_images = batch_images.to(device)\n",
    "\n",
    "#                     # input data -prompt-\n",
    "#                     batch_prompts = val_prompts[i:i+BATCH_SIZE]\n",
    "#                     targets = st_model.encode(batch_prompts,\n",
    "#                                               show_progress_bar=False,\n",
    "#                                               convert_to_tensor=True)\n",
    "\n",
    "#                     preds = model(batch_images)\n",
    "                    \n",
    "#                     labels = torch.ones(targets.size(0)).to(device)\n",
    "#                     loss = criterion(preds, targets, labels)\n",
    "#                     similarity = cosine_similarity(\n",
    "#                         preds.detach().cpu().numpy(), \n",
    "#                         targets.detach().cpu().numpy()\n",
    "#                     )\n",
    "                    \n",
    "#                     val_loss += loss.item()\n",
    "#                     val_similarity += similarity\n",
    "#                 val_loss /= num_batchs\n",
    "#                 val_similarity /= num_batchs\n",
    "#                 print(f'val loss: {val_loss} ', end='')\n",
    "#                 print(f'val similarity: {val_similarity}')\n",
    "\n",
    "#             # Save\n",
    "#             if val_similarity > BEST_SIMILARITY:\n",
    "#                 BESTSIM = val_similarity\n",
    "#                 with open(f'{MODEL_NAME}/BestSim.pickle', 'wb') as f:\n",
    "#                     pickle.dump(BESTSIM, f)\n",
    "\n",
    "#                 BESTEPOCH = epoch + 1\n",
    "#                 print(f\"save best model at {BESTSIM} with epoch {BESTEPOCH}\")\n",
    "#                 if SAVE_MODEL_CKP:\n",
    "#                     torch.save(vit.state_dict(), f\"{MODEL_NAME}/local_30K_model.pt\")\n",
    "#                 if SAVE_OPT_CKP:\n",
    "#                     torch.save(optimizer.state_dict(), f\"{MODEL_NAME}/local_30K_opt.pt\")\n",
    "#                 COUNTER = 0\n",
    "\n",
    "#             else:\n",
    "#                 COUNTER += 1\n",
    "\n",
    "#             # Early Stopping\n",
    "#             wandb.log({\n",
    "#                 'train_loss': train_loss,\n",
    "#                 'train_similarity': train_similarity,\n",
    "#                 'val_loss': val_loss,\n",
    "#                 'val_similarity': val_similarity, \n",
    "#                 'epoch': epoch\n",
    "#             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Base + Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base + Denoising + Early Stopping\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        #----------------------------------------------------------------------\n",
    "        RUN_NAME = '2M_gaussian_3'\n",
    "        SAVE_DIR = './models/clip-vit-large-patch14/fine-tuned'\n",
    "        BEST_SIMILARITY = 0\n",
    "        COUNTER = 0\n",
    "        PATIENCE = 4\n",
    "        SAVE_MODEL_CKP = True\n",
    "        SAVE_OPT_CKP = True\n",
    "        \n",
    "        config = wandb.config\n",
    "        EPOCHS = config.epochs\n",
    "        BATCH_SIZE = config.batch_size\n",
    "        DENOISING = config.denoising\n",
    "        KERNEL = config.kernel\n",
    "        UNFREEZE_START = config.unfreeze_start\n",
    "        LEARNING_RATE = config.learning_rate\n",
    "        WEIGHT_DECAY = config.weight_decay\n",
    "        #----------------------------------------------------------------------\n",
    "        \n",
    "        model = load_pretrained_model(UNFREEZE_START)    \n",
    "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad,\n",
    "                                       model.parameters()),\n",
    "                                lr=LEARNING_RATE,\n",
    "                                weight_decay=WEIGHT_DECAY\n",
    "                                # fused=True\n",
    "                                )\n",
    "        optimizer.zero_grad()\n",
    "        # if config.optimizer == 'sgd':\n",
    "        #     optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9)\n",
    "        # elif config.optimizer == 'adam':\n",
    "        #     optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "        criterion = torch.nn.CosineEmbeddingLoss()\n",
    "        \n",
    "        random.seed(42)\n",
    "        shuffled_train_index = random.sample(range(len(train_images)),\n",
    "                                             k=len(train_images))\n",
    "        \n",
    "        num_batchs = math.ceil(len(train_images) / BATCH_SIZE)\n",
    "        \n",
    "        # train\n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            train_similarity = 0\n",
    "            \n",
    "            print(f'epoch: {epoch}')\n",
    "            for i in tqdm(range(0, len(train_images), BATCH_SIZE)):\n",
    "                indices = shuffled_train_index[i:i+BATCH_SIZE]\n",
    "\n",
    "                # input data -image-\n",
    "                batch_images = []\n",
    "                for index in indices:\n",
    "                    image = denoising(train_images[index], \n",
    "                                      DENOISING,\n",
    "                                      KERNEL)\n",
    "                    image = transform(image)\n",
    "                    batch_images.append(image)\n",
    "                batch_images = torch.stack(batch_images)\n",
    "                batch_images = batch_images.to(device)\n",
    "\n",
    "                # input data -prompt-\n",
    "                prompt_list = []\n",
    "                for index in indices:\n",
    "                    prompt_list.append(train_prompts[index])\n",
    "\n",
    "                targets = st_model.encode(prompt_list, \n",
    "                                                show_progress_bar=False,\n",
    "                                                convert_to_tensor=True)\n",
    "                preds = model(batch_images)\n",
    "\n",
    "                # back probagation\n",
    "                labels = torch.ones(targets.size(0)).to(device)\n",
    "                loss = criterion(preds, targets, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                similarity = cosine_similarity(\n",
    "                    preds.detach().cpu().numpy(), \n",
    "                    targets.detach().cpu().numpy()\n",
    "                    )\n",
    "                train_loss += loss.item()\n",
    "                train_similarity += similarity\n",
    "                similarity = 0\n",
    "            train_loss /= num_batchs\n",
    "            train_similarity /= num_batchs\n",
    "            print(f'train loss: {train_loss} ', end='')\n",
    "            print(f'train similarity: {train_similarity}')\n",
    "            \n",
    "            # val\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_similarity = 0\n",
    "            num_batchs = math.ceil(len(val_images) / BATCH_SIZE)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i in tqdm(range(0, len(val_images), BATCH_SIZE)):\n",
    "\n",
    "                    # input data -image-\n",
    "                    images = val_images[i:i+BATCH_SIZE]\n",
    "                    batch_images = []\n",
    "                    for image in images:\n",
    "                        image = denoising(image,\n",
    "                                          DENOISING,\n",
    "                                          KERNEL)\n",
    "                        image = transform(image)\n",
    "                        batch_images.append(image)\n",
    "                    batch_images = torch.stack(batch_images)\n",
    "                    batch_images = batch_images.to(device)\n",
    "\n",
    "                    # input data -prompt-\n",
    "                    batch_prompts = val_prompts[i:i+BATCH_SIZE]\n",
    "                    targets = st_model.encode(batch_prompts,\n",
    "                                              show_progress_bar=False,\n",
    "                                              convert_to_tensor=True)\n",
    "\n",
    "                    preds = model(batch_images)\n",
    "                    \n",
    "                    labels = torch.ones(targets.size(0)).to(device)\n",
    "                    loss = criterion(preds, targets, labels)\n",
    "                    similarity = cosine_similarity(\n",
    "                        preds.detach().cpu().numpy(), \n",
    "                        targets.detach().cpu().numpy()\n",
    "                    )\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    val_similarity += similarity\n",
    "                val_loss /= num_batchs\n",
    "                val_similarity /= num_batchs\n",
    "                print(f'val loss: {val_loss} ', end='')\n",
    "                print(f'val similarity: {val_similarity}')\n",
    "            \n",
    "            # SAVE\n",
    "            if val_similarity > BEST_SIMILARITY:\n",
    "                os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "                BEST_SIMILARITY = val_similarity\n",
    "\n",
    "                if SAVE_MODEL_CKP:\n",
    "                    torch.save(model.state_dict(),\n",
    "                               f\"{SAVE_DIR}/{RUN_NAME}_model.pt\")\n",
    "                if SAVE_OPT_CKP:\n",
    "                    torch.save(optimizer.state_dict(),\n",
    "                               f\"{SAVE_DIR}/{RUN_NAME}_opt.pt\")\n",
    "                COUNTER = 0\n",
    "\n",
    "            else:\n",
    "                COUNTER += 1\n",
    "\n",
    "            # early stopping\n",
    "            wandb.log({\n",
    "                'train_loss': train_loss,\n",
    "                'train_similarity': train_similarity,\n",
    "                'val_loss': val_loss,\n",
    "                'val_similarity': val_similarity, \n",
    "                'epoch': epoch,\n",
    "                'COUNTER': COUNTER})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-07_01-24-33\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "print(dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7x7ry54y) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silvery-grass-3</strong> at: <a href='https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M/runs/7x7ry54y' target=\"_blank\">https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M/runs/7x7ry54y</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230507_012450-7x7ry54y\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7x7ry54y). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c193245019046f09b342b92e0984c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\emanon\\kaggle\\Stable Diffusion - Image to Prompts\\wandb\\run-20230507_012804-k3ebk82z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M/runs/k3ebk82z' target=\"_blank\">clear-eon-4</a></strong> to <a href='https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M' target=\"_blank\">https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M/runs/k3ebk82z' target=\"_blank\">https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M/runs/k3ebk82z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6872bb15c34e4809ac375664a0e1af25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.5431994411972689 train similarity: 0.4568005839540049\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013e9238cdb847ea871fb624c4c3dfa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.40591378672917683 val similarity: 0.5940862310024072\n",
      "epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2f922b29104b09b55610e2f4b4cc27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.2654997159639993 train similarity: 3.5931670971778438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f619e586b0e467d8b678d84b8b873fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.3786221557458242 val similarity: 0.6213778585515397\n",
      "epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d5b992c929407391a2fe03ac724f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.110370997985204 train similarity: 3.7482957999761917\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991f5a3b778d4ad1981e3293c164bb67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.3728062249024709 val similarity: 0.6271937896047654\n",
      "epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b6382b0c124a04ae5063064f6f37cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.0558510292768477 train similarity: 3.80281576757645\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0afc7748018456292f8240569e65f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.3716648835341136 val similarity: 0.6283351292109697\n",
      "epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df569278de74dee9798751349457341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.0415396473805107 train similarity: 3.8171271450024356\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2f43742e2a47ff8b3b9fb09596084e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.37014477415879565 val similarity: 0.6298552401579619\n",
      "epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86e81638557499ebc78258d37f22c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.0262021905581156 train similarity: 3.832464602082123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a0f97738404bdfbaf5249cd1b06a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.36824745003382364 val similarity: 0.6317525642964573\n",
      "epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea990f86a4d7438c9a0d4fe534182ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.0243666592439014 train similarity: 3.8343001352479837\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70151ed68904557927c909adf680db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.3675593593517939 val similarity: 0.6324406562596417\n",
      "epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f51b5c6ba9d4ac78d3ff9f78c709c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.0203074908653895 train similarity: 3.8383592981729295\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcaae4ac13f4a25b507a7be0ba13a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.36789794143040977 val similarity: 0.6321020734205092\n",
      "epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0273df1c66b942e88ee8a6e5775a06f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 96\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[0;32m     93\u001b[0m     image \u001b[38;5;241m=\u001b[39m denoising(train_images[index], \n\u001b[0;32m     94\u001b[0m                       DENOISING,\n\u001b[0;32m     95\u001b[0m                       KERNEL)\n\u001b[1;32m---> 96\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     batch_images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[0;32m     98\u001b[0m batch_images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(batch_images)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    488\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py:2192\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2184\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2185\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2186\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2187\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2188\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2189\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2190\u001b[0m         )\n\u001b[1;32m-> 2192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Base + Denoising + Logging\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Run parameters\n",
    "RUN_NAME = '2M_gaussian_kernel_3'\n",
    "SAVE_DIR = './models/clip-vit-large-patch14/fine-tuned'\n",
    "SAVE_MODEL_CKP = True\n",
    "SAVE_OPT_CKP = True\n",
    "BEST_SIMILARITY = 0\n",
    "BATCH_SIZE = 30\n",
    "SEED = 42\n",
    "\n",
    "# Early stopping parameters\n",
    "MIN_DELTA = 0.001\n",
    "PATIENCE = 3\n",
    "EPOCHS = 10000\n",
    "\n",
    "# Model & Optimizer parameters\n",
    "UNFREEZE_START = 21\n",
    "LEARNING_RATE = 0.004836150520205524\n",
    "WEIGHT_DECAY = 0.05458727423867926\n",
    "\n",
    "# Preprocessing parameters\n",
    "DENOISING = 'gaussian'\n",
    "KERNEL = 3\n",
    "\n",
    "# notes\n",
    "notes = 'お試し'\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "import wandb\n",
    "import random # for demo script\n",
    "\n",
    "config = {\n",
    "    'unfreeze_start': UNFREEZE_START,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    'min_delta': MIN_DELTA,\n",
    "    'patience': PATIENCE,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'denoising': DENOISING,\n",
    "    'kernel': KERNEL,\n",
    "}\n",
    "\n",
    "# 1\n",
    "wandb.login()\n",
    "\n",
    "# 2\n",
    "run = wandb.init(\n",
    "    project=\"SD_Train_CLIP-ViT-Large_2M\",\n",
    "    config=config,\n",
    "    notes=notes\n",
    ")\n",
    "\n",
    "# 3\n",
    "# Model & Optimizer & Criterion\n",
    "model = load_pretrained_model(UNFREEZE_START)    \n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad,\n",
    "                               model.parameters()),\n",
    "                        lr=LEARNING_RATE,\n",
    "                        weight_decay=WEIGHT_DECAY\n",
    "                        # fused=True\n",
    "                        )\n",
    "optimizer.zero_grad()\n",
    "# if config.optimizer == 'sgd':\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9)\n",
    "# elif config.optimizer == 'adam':\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "criterion = torch.nn.CosineEmbeddingLoss()\n",
    "\n",
    "# train index\n",
    "random.seed(SEED)\n",
    "shuffled_train_index = random.sample(range(len(train_images)),\n",
    "                                     k=len(train_images))\n",
    "\n",
    "# Avaraging parameter\n",
    "num_batchs = math.ceil(len(train_images) / BATCH_SIZE)\n",
    "\n",
    "# Early stopping counter\n",
    "counter = 0\n",
    "\n",
    "# train\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_similarity = 0\n",
    "\n",
    "    print(f'epoch: {epoch}')\n",
    "    for i in tqdm(range(0, len(train_images), BATCH_SIZE)):\n",
    "        indices = shuffled_train_index[i:i+BATCH_SIZE]\n",
    "\n",
    "        # input data -image-\n",
    "        batch_images = []\n",
    "        for index in indices:\n",
    "            image = denoising(train_images[index], \n",
    "                              DENOISING,\n",
    "                              KERNEL)\n",
    "            image = transform(image)\n",
    "            batch_images.append(image)\n",
    "        batch_images = torch.stack(batch_images)\n",
    "        batch_images = batch_images.to(device)\n",
    "\n",
    "        # input data -prompt-\n",
    "        prompt_list = []\n",
    "        for index in indices:\n",
    "            prompt_list.append(train_prompts[index])\n",
    "\n",
    "        targets = st_model.encode(prompt_list, \n",
    "                                        show_progress_bar=False,\n",
    "                                        convert_to_tensor=True)\n",
    "        preds = model(batch_images)\n",
    "\n",
    "        # back probagation\n",
    "        labels = torch.ones(targets.size(0)).to(device)\n",
    "        loss = criterion(preds, targets, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        similarity = cosine_similarity(\n",
    "            preds.detach().cpu().numpy(), \n",
    "            targets.detach().cpu().numpy()\n",
    "            )\n",
    "        train_loss += loss.item()\n",
    "        train_similarity += similarity\n",
    "        similarity = 0\n",
    "    train_loss /= num_batchs\n",
    "    train_similarity /= num_batchs\n",
    "    print(f'train loss: {train_loss} ', end='')\n",
    "    print(f'train similarity: {train_similarity}')\n",
    "\n",
    "    # val\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_similarity = 0\n",
    "    num_batchs = math.ceil(len(val_images) / BATCH_SIZE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(val_images), BATCH_SIZE)):\n",
    "\n",
    "            # input data -image-\n",
    "            images = val_images[i:i+BATCH_SIZE]\n",
    "            batch_images = []\n",
    "            for image in images:\n",
    "                image = denoising(image, DENOISING, KERNEL)\n",
    "                image = transform(image)\n",
    "                batch_images.append(image)\n",
    "            batch_images = torch.stack(batch_images)\n",
    "            batch_images = batch_images.to(device)\n",
    "\n",
    "            # input data -prompt-\n",
    "            batch_prompts = val_prompts[i:i+BATCH_SIZE]\n",
    "            targets = st_model.encode(batch_prompts,\n",
    "                                      show_progress_bar=False,\n",
    "                                      convert_to_tensor=True)\n",
    "\n",
    "            preds = model(batch_images)\n",
    "\n",
    "            labels = torch.ones(targets.size(0)).to(device)\n",
    "            loss = criterion(preds, targets, labels)\n",
    "            similarity = cosine_similarity(\n",
    "                preds.detach().cpu().numpy(), \n",
    "                targets.detach().cpu().numpy()\n",
    "            )\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_similarity += similarity\n",
    "            similarity = 0\n",
    "        val_loss /= num_batchs\n",
    "        val_similarity /= num_batchs\n",
    "        print(f'val loss: {val_loss} ', end='')\n",
    "        print(f'val similarity: {val_similarity}')\n",
    "\n",
    "    # 3\n",
    "    wandb.log({\n",
    "        'train_loss': train_loss,\n",
    "        'train_similarity': train_similarity,\n",
    "        'val_loss': val_loss,\n",
    "        'val_similarity': val_similarity, \n",
    "        'epoch': epoch\n",
    "    })\n",
    "        \n",
    "    # Save & Early stopping\n",
    "    delta = (val_similarity - BEST_SIMILARITY)\n",
    "    print(f'delta: {delta}')\n",
    "    if delta >= MIN_DELTA:\n",
    "        BEST_SIMILARITY = val_similarity\n",
    "        os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "        if SAVE_MODEL_CKP:\n",
    "            torch.save(model.state_dict(),\n",
    "                       f\"{SAVE_DIR}/{RUN_NAME}_model.pt\")\n",
    "        if SAVE_OPT_CKP:\n",
    "            torch.save(optimizer.state_dict(),\n",
    "                       f\"{SAVE_DIR}/{RUN_NAME}_opt.pt\")\n",
    "        counter = 0\n",
    "\n",
    "    else:\n",
    "        counter += 1\n",
    "    \n",
    "    wandb.alert(\n",
    "        title=\"Weights & Biases\", \n",
    "        text=f\"epoch {epoch} のval_similarityは {val_similarity} です。\"\n",
    "    )\n",
    "    \n",
    "    if counter > PATIENCE:\n",
    "        break\n",
    "\n",
    "run.log_code()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mataracsia\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: yqoig2oz\n",
      "Sweep URL: https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M/sweeps/yqoig2oz\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project='SD_Train_CLIP-ViT-Large_2M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9aymi1xj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoising: gaussian\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.004836150520205524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tunfreeze_start: 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.05458727423867926\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\emanon\\kaggle\\Stable Diffusion - Image to Prompts\\wandb\\run-20230505_225551-9aymi1xj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M/runs/9aymi1xj' target=\"_blank\">youthful-sweep-1</a></strong> to <a href='https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M/sweeps/yqoig2oz' target=\"_blank\">https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M/sweeps/yqoig2oz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M' target=\"_blank\">https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M/sweeps/yqoig2oz' target=\"_blank\">https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M/sweeps/yqoig2oz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M/runs/9aymi1xj' target=\"_blank\">https://wandb.ai/ataracsia/SD_Train_CLIP-ViT-Large_2M/runs/9aymi1xj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c642984cce47ff81fb474f8871dae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.5447500800282273 train similarity: 0.45524994517060563\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679ea3de8f444ac2a3465f987174a06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.4070364315509796 val similarity: 0.5929635852376611\n",
      "epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37c6f82736b4f3ca21fe43c993b7221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.2704509987831116 train similarity: 3.588215808037421\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b43b075bc246898f6c7a345235ed87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.3796297113895416 val similarity: 0.6203703040500826\n",
      "epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf413be42d14ba99282b9e47ee3aa49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.110797017852465 train similarity: 3.747869781184552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc31c2f91bb64f9db71291157b7f2b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.373934686700503 val similarity: 0.6260653276214626\n",
      "epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571fdefb91d140b1b43a47ec34975fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.061329373717308 train similarity: 3.7973374242934295\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4d37452da842e2b3571c693c209d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.37185322523117065 val similarity: 0.6281467903271925\n",
      "epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f416cee8159d4c94aab1d0bcc32b0013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.041467283209165 train similarity: 3.817199510430032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f824f31b5d224d9e90d4795b12b516d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.3696058535575867 val similarity: 0.6303941619876554\n",
      "epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8352ccfa40af4d728eba9d9c4abe4490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.028282539288203 train similarity: 3.8303842506055954\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9340386996674a3b90e80675b4c3fa8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.369558451573054 val similarity: 0.6304415622005788\n",
      "epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029a035d32e84d3ab746840a1f8c2422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.0186060134967168 train similarity: 3.8400607790845904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0287776d22be4f66886955acf988281b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.3684655166467031 val similarity: 0.6315344976126177\n",
      "epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2baf7c6ca0dd4d2d8dcd8313bcf3b5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.0136236752669014 train similarity: 3.845043118081741\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a1adb0d8254d7392859a0624aad996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.36783072010676066 val similarity: 0.6321692940091537\n",
      "epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a37ac5b07846e2b74e1b68582b7eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.0087047235568365 train similarity: 3.849962069724182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a1450ce477493a9c9264915b352bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.36644546512762705 val similarity: 0.6335545488812484\n",
      "epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73854c16f5141bdbfe6b31f3cfe3712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.0059936989148457 train similarity: 3.852673092709781\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2711108dd291441f938b7935785bf3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.3675257531007131 val similarity: 0.6324742601150822\n",
      "epoch: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deedd0843eb44830a18180a0bf17e78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.0067986378272376 train similarity: 3.8518681537516617\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fedfd011236c40f89bf4af2fc5fc9871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.36847266781330107 val similarity: 0.6315273462265126\n",
      "epoch: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e97a18f1804c019246a7870be35865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.0021732905308407 train similarity: 3.8564935023277456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52eb0c88dbf241528642dd3311fb88ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.3672534319162369 val similarity: 0.6327465827035786\n",
      "epoch: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ad69167d6b408dadaa80935ec54b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.004393159588178 train similarity: 3.854273634677038\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9d648145d44a29a03d6088152c5930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.3674714574019114 val similarity: 0.6325285573385132\n",
      "epoch: 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8696847ecea40f7826a0071f2c8c7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.0045161658525465 train similarity: 3.854150623219315\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d90d7fc504a4ef09c85a759115aabba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.36805814385414126 val similarity: 0.6319418706640045\n",
      "epoch: 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50006f85d1c4445ab497ed6c3c62e7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train, count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "候補①：\n",
    "\n",
    "wandb: Agent Starting Run: 3pz89orr with config:  \n",
    "wandb: \tbatch_size: 20  \n",
    "wandb: \tepochs: 10  \n",
    "wandb: \tlearning_rate: 0.003942750693801289  \n",
    "wandb: \tunfreeze_start: 22  \n",
    "wandb: \tweight_decay: 0.06609377404182681  \n",
    "\n",
    "Tracking run with wandb version 0.15.0  \n",
    "Run data is saved locally in C:\\Users\\emanon\\kaggle\\Stable Diffusion - Image to Prompts\\wandb\\run-20230501_195702-3pz89orr  \n",
    "Syncing run jolly-sweep-5 to Weights & Biases (docs)  \n",
    "Sweep page: https://wandb.ai/ataracsia/SD_GPT_Method_ViT_large_30K/sweeps/b1jpadmq  \n",
    "View project at https://wandb.ai/ataracsia/SD_GPT_Method_ViT_large_30K  \n",
    "View sweep at https://wandb.ai/ataracsia/SD_GPT_Method_ViT_large_30K/sweeps/b1jpadmq  \n",
    "View run at https://wandb.ai/ataracsia/SD_GPT_Method_ViT_large_30K/runs/3pz89orr  \n",
    "\n",
    "epoch: 0\n",
    "train loss: 0.4756058146158854 train similarity: 0.5243941895369026\n",
    "val loss: 0.5838597163558006 val similarity: 0.41614029235606453\n",
    "\n",
    "epoch: 1\n",
    "train loss: 0.990003741979599 train similarity: 1.2599962616064597\n",
    "val loss: 0.5556601118445397 val similarity: 0.4443398956161023\n",
    "\n",
    "epoch: 2\n",
    "train loss: 0.9432114688158035 train similarity: 1.3067885368624688\n",
    "val loss: 0.5300983499288558 val similarity: 0.46990165720421395\n",
    "\n",
    "epoch: 3\n",
    "train loss: 0.9069464582800865 train similarity: 1.3430535478353498\n",
    "val loss: 0.5118548638820648 val similarity: 0.4881451424020925\n",
    "\n",
    "epoch: 4\n",
    "train loss: 0.8801283758282662 train similarity: 1.3698716284237806\n",
    "val loss: 0.499389152944088 val similarity: 0.5006108502080662\n",
    "\n",
    "epoch: 5\n",
    "train loss: 0.8594652481079101 train similarity: 1.3905347537986936\n",
    "val loss: 0.49192704647779467 val similarity: 0.508072958915308\n",
    "\n",
    "epoch: 6\n",
    "train loss: 0.8414201924800873 train similarity: 1.4085798110809176\n",
    "val loss: 0.48915890061855316 val similarity: 0.510841099741682\n",
    "\n",
    "epoch: 7\n",
    "train loss: 0.8236035017967224 train similarity: 1.42639650161527\n",
    "val loss: 0.4809839709401131 val similarity: 0.5190160317560654\n",
    "\n",
    "epoch: 8\n",
    "train loss: 0.8068020737171173 train similarity: 1.4431979278884843\n",
    "val loss: 0.48496224772930147 val similarity: 0.5150377553672877\n",
    "\n",
    "epoch: 9\n",
    "train loss: 0.7893978360891342 train similarity: 1.4606021691188225\n",
    "val loss: 0.4796355242729187 val similarity: 0.5203644791181199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "候補②：\n",
    "\n",
    "wandb: Agent Starting Run: hiak0yl8 with config:  \n",
    "wandb: \tbatch_size: 20  \n",
    "wandb: \tepochs: 10  \n",
    "wandb: \tlearning_rate: 0.006691501352424112  \n",
    "wandb: \tunfreeze_start: 22  \n",
    "wandb: \tweight_decay: 0.05657430886867784  \n",
    "Tracking run with wandb version 0.15.0  \n",
    "Run data is saved locally in C:\\Users\\emanon\\kaggle\\Stable Diffusion - Image to Prompts\\wandb\\run-20230502_013318-hiak0yl8  \n",
    "Syncing run balmy-sweep-13 to Weights & Biases (docs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the W&B Python Library \n",
    "# import wandb\n",
    "\n",
    "# # 1. Start a W&B Run\n",
    "# run = wandb.init(\n",
    "#   project=\"SD_GPT_Method_ViT_large_30K\",\n",
    "#   notes=\"My first HP-tuning\",\n",
    "#   # tags=[\"baseline\", \"paper1\"]\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
